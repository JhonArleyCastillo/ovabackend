import logging
import asyncio
from gradio_client import Client
from .resilience_service import ResilienceService

logger = logging.getLogger(__name__)

# Lista de endpoints de Hugging Face con fallbacks
GRADIO_ENDPOINTS = [
    {
        "name": "GPT-OSS-20B",
        "endpoint": "merterbak/gpt-oss-20b-demo",
        "api_name": "/chat",
        "supports_system_prompt": True
    },
    {
        "name": "Microsoft DialoGPT",
        "endpoint": "microsoft/DialoGPT-medium",
        "api_name": "/predict",
        "supports_system_prompt": False
    },
    {
        "name": "Hugging Face Chat UI",
        "endpoint": "amd/gpt-oss-120b-chatbot",
        "api_name": "/chat",
        "supports_system_prompt": True
    }
]

DEFAULT_SYSTEM_PROMPT = "Eres un asistente √∫til y amigable. Responde de manera clara y concisa en espa√±ol."

def _create_gradio_client(endpoint: str):
    """Crea una instancia del cliente Gradio para un endpoint espec√≠fico."""
    try:
        return Client(endpoint)
    except Exception as e:
        logger.error(f"Error al crear cliente Gradio para {endpoint}: {e}")
        raise

def _call_gradio_endpoint(endpoint_config: dict, user_input: str, system_prompt: str = None, max_tokens: int = 1024) -> str:
    """Llama a un endpoint espec√≠fico de Gradio."""
    try:
        client = _create_gradio_client(endpoint_config["endpoint"])
        
        if endpoint_config["supports_system_prompt"]:
            # Endpoint que soporta system prompt
            final_system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
            
            result = client.predict(
                input_data=user_input,
                max_new_tokens=max_tokens,
                system_prompt=final_system_prompt,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.0,
                api_name=endpoint_config["api_name"]
            )
        else:
            # Endpoint simple sin system prompt
            result = client.predict(
                user_input,
                api_name=endpoint_config["api_name"]
            )
        
        # Procesar el resultado
        if isinstance(result, str):
            return result
        elif isinstance(result, (list, tuple)) and len(result) > 0:
            return str(result[0])
        else:
            logger.warning(f"Formato de respuesta inesperado de {endpoint_config['name']}: {type(result)}")
            return str(result)
            
    except Exception as e:
        logger.error(f"Error en llamada a {endpoint_config['name']}: {e}")
        raise

def _get_fallback_response(user_input: str) -> str:
    """Genera una respuesta de fallback inteligente cuando todos los endpoints fallan."""
    user_lower = user_input.lower()
    
    # Saludos y presentaciones
    if any(greeting in user_lower for greeting in ["hola", "hello", "hi", "buenos d√≠as", "buenas tardes", "buenas noches"]):
        return "¬°Hola! üëã Soy tu asistente virtual OVA. Aunque el servicio de IA avanzada no est√° disponible en este momento, puedo ayudarte con informaci√≥n b√°sica y responder preguntas simples. ¬øEn qu√© puedo asistirte?"
    
    # Preguntas sobre programaci√≥n
    elif any(term in user_lower for term in ["python", "javascript", "programaci√≥n", "c√≥digo", "html", "css"]):
        return "¬°Me encanta hablar de programaci√≥n! üíª Aunque no tengo acceso al modelo de IA completo ahora, puedo ayudarte con conceptos b√°sicos. Python es un lenguaje de programaci√≥n vers√°til, JavaScript es esencial para web, y HTML/CSS son la base del dise√±o web. ¬øHay algo espec√≠fico que te gustar√≠a saber?"
    
    # Preguntas sobre inteligencia artificial
    elif any(term in user_lower for term in ["inteligencia artificial", "ia", "ai", "machine learning", "aprendizaje autom√°tico"]):
        return "ü§ñ La Inteligencia Artificial es fascinante! Es la tecnolog√≠a que permite a las m√°quinas realizar tareas que normalmente requieren inteligencia humana, como reconocer patrones, tomar decisiones y procesar lenguaje natural. Incluye campos como machine learning, deep learning y redes neuronales. ¬øTe interesa alg√∫n aspecto espec√≠fico?"
    
    # Preguntas generales de conocimiento
    elif any(question in user_lower for question in ["qu√© es", "que es", "c√≥mo", "como", "cu√°l", "cual", "por qu√©", "porque", "why", "what", "how"]):
        if "fotos√≠ntesis" in user_lower:
            return "üå± La fotos√≠ntesis es el proceso por el cual las plantas convierten la luz solar, agua y di√≥xido de carbono en glucosa y ox√≠geno. Es fundamental para la vida en la Tierra porque produce el ox√≠geno que respiramos y es la base de la cadena alimentaria."
        elif "gravity" in user_lower or "gravedad" in user_lower:
            return "üåç La gravedad es la fuerza de atracci√≥n entre objetos con masa. En la Tierra, nos mantiene con los pies en el suelo y hace que los objetos caigan hacia el centro del planeta."
        else:
            return "Es una pregunta interesante! ü§î Aunque no tengo acceso al modelo de IA completo en este momento, puedo intentar ayudarte con informaci√≥n b√°sica. ¬øPodr√≠as reformular tu pregunta de manera m√°s espec√≠fica?"
    
    # Chistes y entretenimiento
    elif any(term in user_lower for term in ["chiste", "joke", "divertido", "gracioso"]):
        chistes = [
            "¬øPor qu√© los programadores prefieren el modo oscuro? Porque la luz atrae bugs! üêõüíª",
            "¬øC√≥mo llamas a un robot que toma el camino m√°s largo? R2-Detr√∫s! ü§ñ",
            "¬øPor qu√© los desarrolladores odian la naturaleza? Porque tiene demasiados bugs! üåøüêõ"
        ]
        import random
        return random.choice(chistes)
    
    # Agradecimientos
    elif any(thanks in user_lower for thanks in ["gracias", "thanks", "thank you", "muchas gracias"]):
        return "¬°De nada! üòä Aunque el servicio completo de IA no est√© disponible, me alegra poder ayudarte con lo que puedo. Si necesitas algo m√°s espec√≠fico, ¬°no dudes en preguntar!"
    
    # Despedidas
    elif any(bye in user_lower for bye in ["adi√≥s", "adios", "bye", "hasta luego", "nos vemos"]):
        return "¬°Hasta luego! üëã Fue un placer ayudarte. Vuelve cuando necesites asistencia. ¬°Que tengas un excelente d√≠a!"
    
    # Consultas sobre el estado del servicio
    elif any(term in user_lower for term in ["funciona", "disponible", "servicio", "error"]):
        return "üîß El servicio de IA avanzada est√° temporalmente no disponible debido a limitaciones de cuota en los servidores externos. Sin embargo, puedo ayudarte con respuestas b√°sicas y informaci√≥n general. ¬øHay algo espec√≠fico en lo que pueda asistirte mientras tanto?"
    
    # Respuesta general
    else:
        return f"He recibido tu mensaje: '{user_input}' üìù\n\nAunque el modelo de IA completo no est√° disponible ahora, estoy aqu√≠ para ayudarte con informaci√≥n b√°sica. Puedo responder preguntas sobre programaci√≥n, ciencia, tecnolog√≠a, o simplemente conversar contigo. ¬øEn qu√© te gustar√≠a que te ayude?"

@ResilienceService.resilient_hf_call(
    timeout_seconds=30.0,  # Reducido para ser m√°s responsivo
    retry_attempts=2,      # Menos intentos para endpoints lentos
    fallback_response="Lo siento, el servicio no est√° disponible en este momento. Por favor, int√©ntalo m√°s tarde."
)
async def get_llm_response_async(user_input: str, system_prompt: str = None, max_tokens: int = 1024) -> str:
    """Obtiene una respuesta usando m√∫ltiples endpoints como fallback."""
    try:
        # Ejecutar la llamada con fallbacks en un thread separado para mantener async
        response = await asyncio.get_event_loop().run_in_executor(
            None, _call_with_fallbacks, user_input, system_prompt, max_tokens
        )
        logger.info(f"Respuesta generada para: '{user_input[:30]}...'")
        return response
    except Exception as e:
        logger.error(f"Error al obtener respuesta: {e}")
        raise

def _call_with_fallbacks(user_input: str, system_prompt: str = None, max_tokens: int = 1024) -> str:
    """Intenta llamar a endpoints con fallbacks autom√°ticos."""
    
    for i, endpoint_config in enumerate(GRADIO_ENDPOINTS):
        try:
            logger.info(f"Intentando endpoint {i+1}/{len(GRADIO_ENDPOINTS)}: {endpoint_config['name']}")
            response = _call_gradio_endpoint(endpoint_config, user_input, system_prompt, max_tokens)
            logger.info(f"‚úÖ Respuesta exitosa de {endpoint_config['name']}")
            return response
            
        except Exception as e:
            logger.warning(f"‚ùå Fall√≥ endpoint {endpoint_config['name']}: {e}")
            
            # Si es el √∫ltimo endpoint, usar fallback local
            if i == len(GRADIO_ENDPOINTS) - 1:
                logger.info("Usando respuesta de fallback local")
                return _get_fallback_response(user_input)
            
            # Continuar con el siguiente endpoint
            continue
    
    # Esto no deber√≠a ejecutarse nunca, pero por seguridad
    return _get_fallback_response(user_input)

def get_llm_response(user_input: str, system_prompt: str = None, max_tokens: int = 1024) -> str:
    """Obtiene una respuesta usando m√∫ltiples endpoints (versi√≥n s√≠ncrona)."""
    try:
        return _call_with_fallbacks(user_input, system_prompt, max_tokens)
    except Exception as e:
        logger.error(f"Error al obtener respuesta: {e}")
        return _get_fallback_response(user_input)

# APIs externas migradas a gradio_client - c√≥digo legacy limpiado 